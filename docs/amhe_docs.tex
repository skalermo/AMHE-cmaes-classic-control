\documentclass[12pt,a4paper]{article}

% for polish language
\usepackage{polski}

% for some math symbols
\usepackage{amssymb}

% correct footnotes placement
\usepackage[bottom]{footmisc}

% for \say command
\usepackage{dirtytalk}

% change title of the bibliography
\def\bibname{Referencje}\let\refname\bibname

% for advanced math formulas
\usepackage{mathtools}
% define ceil and floor
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{listings}
% colors for snippet background
\usepackage{xcolor}

% links
\usepackage{hyperref}
% \hypersetup{
%     colorlinks,
%     citecolor=black,
%     filecolor=black,
%     linkcolor=black,
%     urlcolor=blue
% }

% image displaying
\usepackage{subcaption}
\usepackage{graphicx}

\usepackage{multirow} 
\usepackage{makecell}

% display structure of the project
\usepackage{dirtree}

\title{Dokumentacja do projektu z AMHE}

\author{Roman Moskalenko \and Pavel Tarashkevich}
\date{}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Treść zadania}

\textbf{Zadanie 10.}\\

Zaprojektuj algorytm ewolucyjny i zbadaj jego działanie w jednym z zadań zdefiniowanych w ramach środowiska OpenAI poświęconym kontroli.

Specyfikacja zadań znajduje się pod poniższym linkiem:

\href{https://gym.openai.com/envs/#classic\_control}{gym.openai.com/envs/\#classic\_control}

Porównaj wyniki swojego rozwiązania z dwoma wybranymi rozwiązaniami, które bazują na
innych podejściach.


\section{Projekt wstępny}

\subsection{Opis problemu i jego sposób rozwiązania}

Zadania kontroli klasycznej zakładają sterowanie obiektu fizycznego w sposób,
jak najbardziej efektywny dla jego stanu. Funkcja sterowania może być bardzo
skomplikowana, uciążliwe jest jej zaprojektowanie w sposób ręczny.

Algorytmy uczenia się ze wzmocnieniem (dalej RL) są wykorzystywane w zadaniach związanych z kontrolą.
Do ewaluacji tych algorytmów powstał zestaw środowisk OpenAI gym.

Alternatywnym podejściem do zadań kontroli jest bezpośrednie stosowanie sieci
neuronowych, m.i. podejście neuroewolucji zastosowane w \cite{scalable_alternative},
które zakłada połączenie sieci neuronowych i algorytmów ewolucyjnych.

Do rozwiązania tego problemu planujemy zastosować sieć MLP, gdzie wejściem sieci
będzie stan rozważanego środowiska, a wyjściem będzie akcja do wyboru przez agenta.
Sieć ta będzie optymalizowana za pomocą algorytmu ewolucyjnego CMA-ES.

\subsection{Planowane eksperymenty numeryczne}\label{subsection:planned}

W ramach eksperymentów ocenimy działanie implementacji naszego algorytmu
neuroewolucji na różnych środowiskach OpenAI gym.

Następnie porównamy jego działanie z dwoma algorytmami RL pod kątem
końcowej nagrody oraz czasu uczenia algorytmu. Wybrane przez nas
algorytmy RL to A2C oraz PPO.


\subsection{Biblioteki wybrane do realizacji projektu}\label{subsection:libs}

\begin{itemize}
  \item \textbf{\href{https://github.com/openai/gym}{gym}} - liczne środowiska, w tym kontroli klasycznej,
        do trenowania agentów.
  \item \textbf{\href{https://github.com/CyberAgentAILab/cmaes}{cmaes}} -
        implementacja algorytmu CMA-ES.
  \item \textbf{\href{https://github.com/DLR-RM/stable-baselines3}{stable-baselines}} -
        biblioteka zawierająca gotowe implementację algorytmów RL.
\end{itemize}

\pagebreak
\section{Project końcowy}

\subsection{Uzupełnienie projektu wstępnego}

Na dodatek do bibliotek wymienionych w \ref{subsection:libs} użyliśmy
biblioteki \textbf{\href{https://pytorch.org/}{PyTorch}} do implementacji
perceptronu wielowarstwowego.

\subsection{Przeprowadzone eksperymenty}

Dany rozdział opisuje szczegóły przeprowadzonych w ramach projektu
eksperymentów: opis środowisk na których działają agenty, ich uczenie oraz
wnioski.

\subsubsection{Opis środowisk kontroli klasycznej}

Do przetestowania algorytmów rozważanych w ramach projektu wykorzystane
zostały środowiska OpenAI gym \cite{openai_gym}, w szczególności dotyczące
kontroli klasycznej: środowiska powszechnie znane, proste ze względu
na działanie i rozwiązywalność.

W tabeli \ref{table:env_properties} znajdują się rozważane przez nas
środowiska oraz ich właściwości.


% 500 iterations
\begin{table}[!h]
  \begin{tabular}{lrrrr}
    \hline
    \multicolumn{1}{c}{Środowisko} & \multicolumn{1}{c}{\# obs.} & \multicolumn{1}{c}{\# akcje} & \multicolumn{1}{c}{typ akcji} & \multicolumn{1}{c}{rozwiązanie} \\
    \hline
    CartPole-v0                    & 4                           & 2                            & dyskretna                     & 195                             \\
    CartPole-v1                    & 4                           & 2                            & dyskretna                     & 475                             \\
    MountainCar-v0                 & 2                           & 3                            & dyskretna                     & -110                            \\
    MountainCarContinuous-v0       & 2                           & 1                            & ciągła                        & 90                              \\
    Pendulum-v1                    & 3                           & 1                            & ciągła                        & -140                            \\
    Acrobot-v1                     & 6                           & 3                            & dyskretna                     & -60                             \\
    \hline
  \end{tabular}
  \caption{
    Właściwości środowisk kontroli klasycznej. Kolumny "obs." oraz "akcje"
    wskazują wymiary zbiorów obserwacji oraz akcji odpowiednio w każdym środowisku.
    Kolumna "rozwiązanie" zawiera wartości, które wskazują granice
    nagród dla każdego środowiska, powyżej których można uznać, że agent
    zachowuje się sensownie \cite{analyzing_reinforcement} \cite{openai_gym}.
  }\label{table:env_properties}
\end{table}

\subsubsection{Metryki do porównania algorytmów}

Zgodnie z rozdziałem \ref{subsection:planned} porównujemy
algorytm neuroewolucji - połączenie CMA-ES z siecią MLP z dwoma algorytmami
RL - A2C oraz PPO.

Zdajemy sobie sprawę, że nie możemy wprost porównywać tych algorytmów, gdyż
charakter uczenia w nich jest różny: algorytm neuroewolucji uczy się
przeszukując przestrzeń parametrów sieci MLP, jedyną informacją zwrotną jest
końcowa suma nagród otrzymana na danym środowisku. Natomiast algorytmy
RL optymalizują swoją politykę zachowania na środowisku, potrafią
uwzględniać natychmiastowe nagrody, a także stany i akcje, które do nich
prowadzą.

Do porównania algorytmów uwzględnimy jak szybko algorytmy osiągają
kolejne sumy nagród, ile czasu zajęło uczenie. Najważniejszą metryką
porównania jest uśredniona suma nagród uzyskiwana  przez modele nauczone.

\subsubsection{Opis modeli do nauczenia}

\textbf{Algorytm neuroewolucji}: użyliśmy perceptronu
dwuwarstwowego z 8 neuronami w warstwie ukrytej. Także perceptron ten
zawiera dodatkowe parametry (bias). Wybraliśmy ReLU jako funkcję aktywacji.
Algorytm \mbox{CMA-ES} automatycznie dobiera liczność populacji zgodnie
ze wzorem:

\begin{equation*}
  population\_size = 4 + \floor{3 * \ln{(n)} }
\end{equation*}

gdzie $n$ to wymiar przestrzeni parametrów sieci. Przykładowo
dla $ n = 100 $ liczność populacji wynosi 17. Parametr $\sigma$
został wybrany arbitralnie i \mbox{wynosi $1.3$}.

\bigskip

\noindent\textbf{Algorytmy A2C i PPO}: użyliśmy domyślnych konfiguracji
modeli tych algorytmów z pakietu stable\_baselines3 z polityką
\emph{MlpPolicy}.

\subsubsection{Uczenie modeli}

\subsubsection{Wnioski}

\subsection{Opis struktury projektu}

Na rysunku \ref{fig:dirtree} pokazana jest struktura projektu.

\begin{figure}[!ht]
  \dirtree{%
    .1 ..
    .2 .gitignore.
    .2 docs.
    .3 amhe\_docs.tex.
    .2 main.py.
    .2 requirements.txt.
    .2 src.
    .3 cmaes\_nn.py.
    .3 env\_info.py.
    .3 log\_utils.py.
    .3 nn.py.
    .2 test.
    .3 test\_cmaes\_nn.py.
    .3 test\_data.
    .4 example\_log\_a2c.txt.
    .4 example\_log\_cmaesnn.txt.
    .3 test\_log\_extraction.py.
    .3 test\_log\_utils.py.
    .3 test\_nn.py.
  }
  \caption{Struktura plików projektu}
  \label{fig:dirtree}
\end{figure}

Plik \textbf{main.py} odpowiada za główny skrypt, który wykonuje uczenie
modeli oraz zapisanie wyników uczenia: logów oraz wytrenowanych modeli
do katalogów \textbf{.data/logs} oraz \textbf{.data/models} odpowiednio. \\

Kod źródłowy projektu, z którego jest zbudowany \textbf{main.py} znajduje się
w katalogu \textbf{src}:
\begin{itemize}
  \item \textbf{cmaes\_nn.py} - zawiera implementację algorytmu do rozwiązywania
        problemów kontroli klasycznej, jego interfejs jest zbliżony
        do algorytmów RL z pakietu \textbf{stable-baselines3}
        (tzn. posiada implementacje funkcji \emph{learn()}, \emph{predict()},
        \emph{load()}, \emph{save()}). \\
        Łączy w sobie stosowanie algorytmu CMA-ES oraz perceptronu wielowarstwowego.

  \item \textbf{env\_info.py} - zawiera listę nazw środowisk do problemów kontroli
        klasycznej oraz informacje o tym, czy typ akcji jest ciągły bądź dyskretny.

  \item \textbf{log\_utils.py} - zawiera funkcje pomocnicze do przetwarzania
        logów programu.

  \item \textbf{nn.py} - zawiera funkcje do zbudowania perceptronu
        wielowarstwowego, a także funkcje do przypisania wektora parametrów
        do wag perceptronu.
\end{itemize}

Wraz z implementacją projektu powstawały testy do sprawdzenia poprawności
jego działania. Znajdują się w katalogu \textbf{test}. \\

Katalog \textbf{docs} przechowuje dokumentację projektu.
Plik \textbf{requirements.txt} zawiera listę wymaganych do zainstalowania
bibliotek.

\subsection{Uruchomienie projektu}

Dla uruchomienia projektu niezbędne jest posiadania interpretera Python wersji co najmniej 3.8
oraz zainstalowanie bibliotek znajdujących w pliku \textbf{requirements.txt}:

\begin{lstlisting}[
  backgroundcolor = \color{lightgray},
  language=bash,
]
  $ pip install -r requirements.txt
\end{lstlisting}

\bigskip

Następnie można uruchomić główny skrypt za pomocą polecenia:

\begin{lstlisting}[
  backgroundcolor = \color{lightgray},
  language=bash,
]
  $ python main.py
\end{lstlisting}

Testy można uruchomić poleceniem:

\begin{lstlisting}[
  backgroundcolor = \color{lightgray},
  language=bash,
]
  $ python -m unittest discover test
\end{lstlisting}

% \appendix
% \section{Wykresy uczenia modeli}

\pagebreak
\begin{thebibliography}{9}
  \bibitem{scalable_alternative}
  Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, Ilya Sutskever,\\
  Evolution Strategies as a Scalable Alternative to Reinforcement Learning,
  2017, \href{https://arxiv.org/pdf/1703.03864v1.pdf}{https://arxiv.org/pdf/1703.03864v1.pdf}

  \bibitem{analyzing_reinforcement}
  Declan Oller, Tobias Glasmachers, Giuseppe Cuccu, \\
  Analyzing Reinforcement Learning Benchmarks with Random Weight Guessing,
  2020, \href{https://arxiv.org/pdf/2004.07707.pdf}{https://arxiv.org/pdf/2004.07707.pdf}

  \bibitem{openai_gym}
  Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba,\\
  OpenAI Gym, 2016, \href{https://gym.openai.com/}{https://gym.openai.com/} \\
  rejestr środowisk (zawiera m.i. nagrody graniczne środowisk): \\
  \href{https://github.com/openai/gym/blob/master/gym/envs/\_\_init\_\_.py}{https://github.com/openai/gym/blob/master/gym/envs/\_\_init\_\_.py}

\end{thebibliography}

\end{document}
